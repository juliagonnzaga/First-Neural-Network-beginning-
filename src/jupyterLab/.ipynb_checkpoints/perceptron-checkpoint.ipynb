{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Perceptron from Scratch \n",
    "\n",
    "The Perceptron is the predecessor of the Multilayer Perceptron (MLP) Artificial Neural Networks. It is a well known, bio-inspired algorithm to do supervised learning. It works as a linear classifier, as we can see in the image:\n",
    "\n",
    "![A simple Perceptron graphic description](\"jupyterLab/imgs/perceptron.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the code we did on the workshop. It might be a computational version of the neuron represented by this image. \n",
    "First of all, we need to import the packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "\n",
    "    def __init__(self, n_input, alpha=0.01):\n",
    "        self.bias_weight = random.uniform(-1, 1)\n",
    "        self.alpha = alpha\n",
    "        self.weights = []\n",
    "        for i in range(n_input):\n",
    "            self.weights.append(random.uniform(-1, 1))\n",
    "        \n",
    "    def classify(self, input):\n",
    "        summation = 0\n",
    "        summation += self.bias_weight * 1\n",
    "        for i in range(len(self.weights)):\n",
    "            summation += self.weights[i] * input[i]\n",
    "        return self.activation(summation)\n",
    "\n",
    "    def activation(self, value):\n",
    "        if(value < 0):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def train(self, input, target):\n",
    "        guess = self.classify(input)\n",
    "        error = target - guess\n",
    "        self.bias_weight += 1 * error * self.alpha\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] += input[i] * error * self.alpha\n",
    "        return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create and test this neuron, althought it basicaly was created at random and the best we can expect from it is a random classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(1)\n",
    "perceptron.classify([10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the same perceptron will always give us the same answer, unless we train it again (and, therefore, update its **weights**). The following cells can show it easily:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always the same values for the same perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(perceptron.classify([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we create new perceptrons (remember that those perceptrons have their wheights initialized randomly). We **may** have different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    newPerceptron = Perceptron(1)\n",
    "    print(newPerceptron.classify([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try training our neuron. Notice that it have just one input and one output, so our classifier would be able to work in just one dimension. Let's say we want a classifier to tell us if a number is less than 50. We should start by creating a proper database, in our case we are using two lists to keep the database: \n",
    "\n",
    "- `X` : a list of lists representing the parameters ou features\n",
    "- `y` : a list with the expected outputs\n",
    "\n",
    "For our \"less than 50\" classifier, we need something like this:\n",
    "\n",
    "| `X` | `y` |\n",
    "|-----|-----|\n",
    "| 10  |  1  |\n",
    "| 30  |  1  |\n",
    "| 55  |  0  |\n",
    "| 100 |  0  |\n",
    "| 20  |  1  |\n",
    "\n",
    "Yes, we could create those two lists by hand. But since we already know the function behind those values, we might want to avoid this trouble and just populate it using a foor loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(80):\n",
    "    x = random.randrange(0,100)\n",
    "    X.append([x])\n",
    "    if x < 50:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our neuron over a few interations with this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(100):\n",
    "    for i in range(len(X)):\n",
    "        perceptron.train(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check if our neuron learned to classify if a number is less than 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    x = random.randrange(0,100)\n",
    "    print(x,\" is less than 50? \", perceptron.classify([x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even after 100 interations we may not have a exact classifier. In this case, it is might happen due to our limited dataset. We have a dataset with 80 examples and probably we are not covering all the possible cases. \n",
    "\n",
    "> #### **Remember: Your classifier is as good as the data you fed it.\n",
    "\n",
    "Let's take a deep look into our classifier and try to find where lies its boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(45,55):\n",
    "    print(x, \" is less than 50? \", perceptron.classify([x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, do we really need the 100 iterations to train this neuron?\n",
    "Let's see how our neuron is learning during the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(1)  \n",
    "errors = []\n",
    "\n",
    "for iteration in range(100):\n",
    "    iteration_error = 0\n",
    "    for i in range(len(X)):\n",
    "        current_error = perceptron.train(X[i], y[i])\n",
    "        iteration_error = iteration_error + current_error\n",
    "    errors.append(iteration_error/len(X))\n",
    "    \n",
    "for i in range(len(errors)):\n",
    "    plt.plot(i, errors[i], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum, so we did a lot of unnecessary training. How we could optimize it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(1)  \n",
    "errors = []    \n",
    "\n",
    "iteration_errors = 1\n",
    "while(iteration_errors!=0):\n",
    "    iteration_errors = 0\n",
    "    for i in range(len(X)):\n",
    "        current_error = perceptron.train(X[i], y[i])\n",
    "        iteration_errors = iteration_errors + current_error\n",
    "    errors.append(iteration_errors/len(X))\n",
    "    \n",
    "for i in range(len(errors)):\n",
    "    plt.plot(i, errors[i], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum... Do you remember we had talked about how the perceptron is close to a linear function? What about plot it the perceptron behaviour and see what we can find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func_linear(x1, a, b):\n",
    "    return a * x1 + b\n",
    "\n",
    "X = np.arange(-100, 100, 0.5)\n",
    "Y = func_linear(X, perceptron.weights[0], perceptron.bias_weight)\n",
    "plt.plot(X, Y, '--')\n",
    "Y2 = np.heaviside(-Y, 0)\n",
    "plt.plot(X, Y2, '--')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to move to 2 inputs? \n",
    "We used a list to represent our inputs exactly to do such a thing, we are able to have as many inputs as we want.\n",
    "\n",
    "As a exemple, we may try to train a perceptron to learn the **OR** table:\n",
    "\n",
    "x1 | x2 | y |\n",
    "-- | -- | --|\n",
    " 0 |  0 | 0 |\n",
    " 0 |  1 | 1 |\n",
    " 1 |  0 | 1 |\n",
    " 1 |  1 | 1 |\n",
    " \n",
    " Let's define our \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "y = [0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can visualize it before try to train the neuron. It may be a helpful step, once we already know the perceptron's limitations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y)):\n",
    "    dot = 'ko' if y[i] > 0 else 'ro'\n",
    "    plt.plot(X[i][0], X[i][1], dot)\n",
    "plt.axis([-0.2, 1.2, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and train our perceptron:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron(2)\n",
    "\n",
    "for iteration in range(100):\n",
    "    for i in range(len(X)):\n",
    "        perceptron.train(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it and find if the neuron really learned the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    print(X[i], perceptron.classify(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_y(fig, y):\n",
    "    for i in range(len(y)):\n",
    "        dot = 'ko' if y[i] > 0 else 'ro'\n",
    "        plt.plot(X[i][0], X[i][1], dot)\n",
    "    return fig\n",
    "\n",
    "def plot_perceptron_func(p, y):\n",
    "    h = .02\n",
    "    x_min = -0.8\n",
    "    x_max = 1.2\n",
    "    y_min = -0.8\n",
    "    y_max = 1.2\n",
    "    xx = np.arange(x_min, x_max, h)\n",
    "    yy = np.arange(y_min, y_max, h)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig = plot_y(fig, y)\n",
    "    Z = []\n",
    "    for x in xx:\n",
    "        z = []\n",
    "        for y in yy:\n",
    "            z.append(p.classify([x, y]))\n",
    "        Z.append(z)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    plt.axis([-0.2, 1.2, -0.2, 1.2])\n",
    "#     plt.show()\n",
    "    \n",
    "plot_perceptron_func(perceptron, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
